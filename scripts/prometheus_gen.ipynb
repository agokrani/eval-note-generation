{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7RCyVkqWYHcV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701547649920,"user_tz":-60,"elapsed":18036,"user":{"displayName":"Karim Mussa","userId":"08025975144684658756"}},"outputId":"fa64f375-8087-4eb2-f650-9ec45234b220"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n"]}],"source":["!pip install transformers\n","!pip install huggingface_hub"]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6B0ohhduYnEV","executionInfo":{"status":"ok","timestamp":1701547666452,"user_tz":-60,"elapsed":7513,"user":{"displayName":"Karim Mussa","userId":"08025975144684658756"}},"outputId":"3efbe919-2db0-48ef-8504-63a9e3678aaf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Token: \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["!pip install accelerate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5BJUidhjY8a4","executionInfo":{"status":"ok","timestamp":1701547676715,"user_tz":-60,"elapsed":7839,"user":{"displayName":"Karim Mussa","userId":"08025975144684658756"}},"outputId":"4becc1d0-0bab-44dc-8e62-e97d7e42b25e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate\n","  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/265.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/265.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.25.0\n"]}]},{"cell_type":"code","source":["!pip install accelerate\n","!pip install -i https://test.pypi.org/simple/ bitsandbytes\n","!pip install transformers==4.30\n","!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pcc826SCAKuB","executionInfo":{"status":"ok","timestamp":1701547732045,"user_tz":-60,"elapsed":55335,"user":{"displayName":"Karim Mussa","userId":"08025975144684658756"}},"outputId":"47febacc-bdd8-4003-8957-e0a763152d35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Looking in indexes: https://test.pypi.org/simple/\n","Collecting bitsandbytes\n","  Downloading https://test-files.pythonhosted.org/packages/5c/e0/597d593ec3b6cf5ea7eb4894a545045bd95611de8a316a2a1eaa838a2459/bitsandbytes-0.39.0-py3-none-any.whl (95.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.39.0\n","Collecting transformers==4.30\n","  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (2023.11.17)\n","Installing collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.15.0\n","    Uninstalling tokenizers-0.15.0:\n","      Successfully uninstalled tokenizers-0.15.0\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.35.2\n","    Uninstalling transformers-4.35.2:\n","      Successfully uninstalled transformers-4.35.2\n","Successfully installed tokenizers-0.13.3 transformers-4.30.0\n","Collecting datasets\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting pyarrow-hotfix (from datasets)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n","Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"u0TMT5GA2dEh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701549716552,"user_tz":-60,"elapsed":16383,"user":{"displayName":"Aman Gokrani","userId":"08588025499976632515"}},"outputId":"dcaafd0b-7add-43c2-b714-ef914ccf6695"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, LlamaForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n","model = LlamaForCausalLM.from_pretrained(\"kaist-ai/Prometheus-7b-v1.0\", cache_dir=\"/content/drive/MyDrive/models\", load_in_4bit=True)\n","tokenizer.pad_token = tokenizer.eos_token"],"metadata":{"id":"HsWzFHsfYyA2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!git clone https://github.com/agokrani/eval-note-generation.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wC5wsQJ56CAK","executionInfo":{"status":"ok","timestamp":1701440564759,"user_tz":-60,"elapsed":886,"user":{"displayName":"Aman Gokrani","userId":"08588025499976632515"}},"outputId":"f1b3a644-9c9e-45fa-9a56-a64fb8824e90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'eval-note-generation'...\n","remote: Enumerating objects: 12, done.\u001b[K\n","remote: Counting objects: 100% (12/12), done.\u001b[K\n","remote: Compressing objects: 100% (9/9), done.\u001b[K\n","remote: Total 12 (delta 1), reused 9 (delta 1), pack-reused 0\u001b[K\n","Receiving objects: 100% (12/12), 198.02 KiB | 2.36 MiB/s, done.\n","Resolving deltas: 100% (1/1), done.\n"]}]},{"cell_type":"code","source":["import os\n","import re\n","import json\n","from datasets import load_dataset\n","from string import Template"],"metadata":{"id":"_nOwlKJuw_MZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATASET_PATH = '/content/drive/MyDrive/git/eval-note-generation/outputs/train.gpt-4-1106-preview.pred.fullnote.json'\n","TEMP_PATH = '/content/drive/MyDrive/git/eval-note-generation/outputs/temp/prompt-1'\n","dataset = load_dataset('json', data_files=DATASET_PATH)\n","dataset = dataset['train']"],"metadata":{"id":"1NtBOsg965e5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PROMPT = Template(\n","\"\"\"\n","###Task Description:\n","An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n","1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n","2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n","3. The output format should look as follows: \\\"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\\\"\n","4. Please do not generate any other opening, closing, and explanations.\n","\n","###The instruction to evaluate:\n","summarize the conversation to generate a clinical note with four sections: HISTORY OF PRESENT ILLNESS, PHYSICAL EXAM, RESULTS, ASSESSMENT AND PLAN.\n","The conversation is:\n","$conversation\n","\n","###Response to evaluate:\n","$summary\n","\n","###Reference Answer (Score 5):\n","$reference\n","\n","###Score Rubric:\n","[Is the model able to accurately and effectively summarize a medical conversation into a clinical note with four sections: HISTORY OF PRESENT ILLNESS, PHYSICAL EXAM, RESULTS, ASSESSMENT AND PLAN?]\n","\n","Score 1: The summary utterly fails to reflect the conversation. It is incoherent, irrelevant, excessively verbose, or filled with hallucinations. There is a blatant disregard for standard clinical terminology, and critical information is omitted.\n","\n","Score 2: The summary sporadically reflects elements of the conversation, but it frequently includes irrelevant or incoherent content. There is a noticeable lack of standard clinical terminology, verbosity is apparent, hallucinations are present, and critical information is often omitted.\n","\n","Score 3: The summary generally captures the conversation accurately but occasionally includes irrelevant or incoherent content. It mostly uses standard clinical terms but can be verbose at times. Minor hallucinations may occur, and there might be instances of critical information being overlooked.\n","\n","Score 4: The summary often accurately reflects the conversation, maintaining coherence and relevance throughout. There are minor cases of verbosity or use of non-standard clinical terms. The summary may have slight omissions or infrequent minor hallucinations.\n","\n","Score 5: The summary flawlessly encapsulates the conversation, demonstrating complete coherence, relevance, and succinctness. It consistently employs standard clinical terminology, contains no hallucinations, and does not omit any critical information.\n","\n","###Feedback:\n","\"\"\"\n",")"],"metadata":{"id":"8e-IAzRN94lD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parse_feedback(input_string):\n","    \"\"\"\n","    Parses the string to extract the content that comes after '###Feedback:'.\n","\n","    Args:\n","    input_string (str): The input string containing the '###Feedback:' marker.\n","\n","    Returns:\n","    str: The substring that comes after '###Feedback:'.\n","    \"\"\"\n","    # Define the marker\n","    marker = \"###Feedback:\"\n","\n","    # Find the index of the marker\n","    index = input_string.find(marker)\n","\n","    # Check if the marker is found\n","    if index != -1:\n","        # Extract and return the substring that comes after the marker\n","        return input_string[index + len(marker):].strip()\n","    else:\n","        # Return an empty string if the marker is not found\n","        return \"\"\n"],"metadata":{"id":"h9e92zrUIfQk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for index, example in enumerate(dataset):\n","  temp_file = os.path.join(TEMP_PATH, f\"{example['file']}.json\")\n","  print(\"processing file: \", temp_file)\n","  if not os.path.exists(temp_file):\n","    conversation = example['src'].replace('[doctor]', 'doctor:').replace('[patient]', 'patient:')\n","    conversation = re.sub(r'\\s+([,.?])', r'\\1', conversation)\n","    reference = example['tgt']\n","    summary = example['pred']\n","\n","    #PROMPT_WITH_CON = PROMPT.substitute(conversation=conversation, reference=reference, summary=summary)\n","    PROMPT_WITHOUT_CON = PROMPT.substitute(conversation='', reference=reference, summary=summary)\n","    #input_text = [PROMPT_WITH_CON, PROMPT_WITHOUT_CON]\n","    input_text = PROMPT_WITHOUT_CON\n","    input_ids = tokenizer(input_text, padding=True, return_tensors=\"pt\").input_ids\n","    outputs = model.generate(input_ids, temperature=1.0, top_p=0.4, max_new_tokens=512, repetition_penalty=1.03)\n","    outputs = parse_feedback(tokenizer.decode(outputs[0]))\n","\n","    with open(temp_file, \"w\") as f:\n","      json.dump([outputs], f, indent=4)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMT83x927vNi","executionInfo":{"status":"ok","timestamp":1701458685522,"user_tz":-60,"elapsed":227386,"user":{"displayName":"Aman Gokrani","userId":"08588025499976632515"}},"outputId":"fe42c8ad-7a15-47e6-f62a-5e4ef78d506f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N001-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N002-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N003-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N004-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N005-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N006-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N007-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N008-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N009-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N010-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N011-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N012-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N013-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N014-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N015-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N016-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N017-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N018-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N019-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N020-virtassist.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N021-virtscribe.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N022-virtscribe.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N023-virtscribe.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N024-virtscribe.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N025-virtscribe.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N026-virtscribe.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N027-virtscribe.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N028-virtscribe.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N029-virtscribe.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N030-virtscribe.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N031-virtscribe.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N032-virtscribe.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N033-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N034-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N035-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N036-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N037-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N038-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N039-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N040-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N041-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N042-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N043-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N044-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N045-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N046-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N047-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N048-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N049-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N050-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N051-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N052-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N053-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N054-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N055-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N056-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N057-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N058-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N059-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N060-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N061-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N062-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N063-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N064-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N065-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N066-aci.json\n","processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/D2N067-aci.json\n"]}]},{"cell_type":"code","source":["!nvidia-smi\n","!nvidia-top"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHSeBBqVJLeJ","executionInfo":{"status":"ok","timestamp":1701530948668,"user_tz":-60,"elapsed":427,"user":{"displayName":"Aman Gokrani","userId":"08588025499976632515"}},"outputId":"eb15b49f-d5ec-46a9-8e8d-cca2a51f69c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Dec  2 15:29:07 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   69C    P0    30W /  70W |  14539MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","/bin/bash: line 1: nvidia-top: command not found\n"]}]},{"cell_type":"code","source":["#for index, example in enumerate(dataset):\n","for index in range(20, len(dataset)):\n","  example = dataset[index]\n","  temp_file = os.path.join(TEMP_PATH, f\"{example['file']}.json\")\n","  print(\"processing file: \", temp_file)\n","  if os.path.exists(temp_file):\n","    conversation = example['src'].replace('[doctor]', 'doctor:').replace('[patient]', 'patient:')\n","    conversation = re.sub(r'\\s+([,.?])', r'\\1', conversation)\n","    reference = example['tgt']\n","    summary = example['pred']\n","\n","    PROMPT_WITH_CON = PROMPT.substitute(conversation=conversation, reference=reference, summary=summary)\n","    input_text = PROMPT_WITH_CON\n","    input_ids = tokenizer(input_text, padding=True, return_tensors=\"pt\").input_ids\n","    outputs = model.generate(input_ids, temperature=1.0, top_p=0.4, max_new_tokens=512, repetition_penalty=1.03)\n","    outputs = parse_feedback(tokenizer.decode(outputs[0]))\n","\n","    with open(temp_file, \"r\") as file:\n","      data = json.load(file)\n","    if isinstance(data, list):\n","      # Append the new item to the list\n","      data.append(outputs)\n","    with open(temp_file, \"w\") as file:\n","      json.dump(data, file, indent=4)\n","\n"],"metadata":{"id":"KczzNG_b64-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example = dataset[0]\n","temp_file = os.path.join(TEMP_PATH, f\"{example['file']}.json\")\n","print(\"processing file: \", temp_file)\n","if os.path.exists(temp_file):\n","  conversation = example['src'].replace('[doctor]', 'doctor:').replace('[patient]', 'patient:')\n","  conversation = re.sub(r'\\s+([,.?])', r'\\1', conversation)\n","  reference = example['tgt']\n","  summary = example['pred']\n","\n","  PROMPT_WITH_CON = PROMPT.substitute(conversation=conversation, reference=reference, summary=summary)\n","  input_text = PROMPT_WITH_CON\n","  input_ids = tokenizer(input_text, padding=True, return_tensors=\"pt\").input_ids\n","  outputs = model.generate(input_ids, temperature=1.0, top_p=0.4, max_new_tokens=512, repetition_penalty=1.03)\n","  outputs = parse_feedback(tokenizer.decode(outputs[0]))\n","\n","  with open(temp_file, \"r\") as file:\n","    data = json.load(file)\n","  if isinstance(data, list):\n","    # Append the new item to the list\n","    data.append(outputs)\n","  print(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6OHNzbqBtarx","executionInfo":{"status":"ok","timestamp":1701524157669,"user_tz":-60,"elapsed":74250,"user":{"displayName":"Aman Gokrani","userId":"08588025499976632515"}},"outputId":"9af752c1-f772-426d-8dd8-acd1352009bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["processing file:  /content/drive/MyDrive/git/eval-note-generation/outputs/temp/prompt-1/D2N001-virtassist.json\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[\"The response accurately reflects the conversation, maintaining coherence and relevance throughout. It employs standard clinical terminology and does not contain any hallucinations. However, there are instances of verbosity, such as the inclusion of unnecessary details about the patient's travel history and hiking activities. The summary could have been more concise while still providing the necessary information. Additionally, the response could have better emphasized the patient's inconsistent medication adherence and the need for improved adherence. Overall, the response demonstrates a good understanding of the conversation but could have been more concise and focused on the essential points. So the overall score is 4. [RESULT] 4</s>\", 'The summary does not accurately reflect the conversation. It includes irrelevant and incoherent content, and there is a noticeable lack of standard clinical terminology. The verbosity is apparent, and there are instances of critical information being overlooked. The summary does not employ the standard clinical terms, and there are no hallucinations, and the summary does not include all the critical information.\\n\\n\\nSo the overall score is 1.</s>']\n"]}]},{"cell_type":"code","source":["input_text = \"\"\"###Task Description:\n","An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n","1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n","2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n","3. The output format should look as follows: \\\"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\\\"\n","4. Please do not generate any other opening, closing, and explanations.\n","\n","###The instruction to evaluate:\n","summarize the conversation to generate a clinical note with four sections: HISTORY OF PRESENT ILLNESS, PHYSICAL EXAM, RESULTS, ASSESSMENT AND PLAN.\n","The conversation is:\n","doctor: hi, martha. how are you?\n","patient: i'm doing okay. how are you?\n","doctor: i'm doing okay. so, i know the nurse told you about dax. i'd like to tell dax a little bit about you, okay?\n","patient: okay.\n","doctor: martha is a 50-year-old female with a past medical history significant for congestive heart failure, depression and hypertension who presents for her annual exam. so, martha, it's been a year since i've seen you. how are you doing?\n","patient: i'm doing well. i've been traveling a lot recently since things have, have gotten a bit lighter. and i got my, my vaccine, so i feel safer about traveling. i've been doing a lot of hiking. uh, went to washington last weekend to hike in northern cascades, like around the mount baker area.\n","doctor: nice. that's great. i'm glad to hear that you're staying active, you know. i, i just love this weather. i'm so happy the summer is over. i'm definitely more of a fall person.\n","patient: yes, fall foliage is the best.\n","doctor: yeah. um, so tell me, how are you doing with the congestive heart failure? how are you doing watching your diet? i know we've talked about watching a low sodium diet. are you doing okay with that?\n","patient: i've been doing well with that. i resisted, as much, as i could, from the tater tots, you know, the soft pretzels, the salty foods that i, i love to eat. and i've been doing a really good job.\n","doctor: okay, all right. well, i'm glad to hear that. and you're taking your medication?\n","patient: yes.\n","doctor: okay, good. and any symptoms like chest pains, shortness of breath, any swelling in your legs?\n","patient: no, not that i've noticed.\n","doctor: okay, all right. and then in terms of your depression, i know that we tried to stay off of medication in the past because you're on medications for your other problems. how are you doing? and i know that you enrolled into therapy. is that helping? or-\n","patient: yeah, it's been helping a lot. i've been going every week, um, for the past year since my last annual exam. and that's been really helpful for me.\n","doctor: okay. so, no, no issues, no feelings of wanting to harm yourself or hurt others?\n","patient: no, nothing like that.\n","doctor: okay, all right. and then in terms of your high blood pressure, i know that you and i have kind of battled in the past with you remembering to take some of your blood pressure medications. how are you doing with that?\n","patient: i'm still forgetting to take my blood pressure medication. and i've noticed when work gets more stressful, my blood pressure goes up.\n","doctor: okay. and, and so how has work going for you?\n","patient: it's been okay. it's been a lot of long hours, late nights. a lot of, um, you know, fiscal year end data that i've been having to pull. so, a lot of responsibility, which is good. but with the responsibility comes the stress.\n","doctor: yeah, okay, all right. i understand. um, all right. well, i know that you did a review of system sheet when you checked in with the nurse. i know that you were endorsing some nasal congestion from some of the fall pollen and allergies. any other symptoms, nausea or vomiting, abdominal pain, anything like that?\n","patient: no, nothing like that.\n","doctor: no, okay, all right. well, i'm gon na go ahead and do a quick physical exam, okay?\n","patient: okay.\n","doctor: hey, dragon, show me the blood pressure. so, yeah, looking at your blood pressure today here in the office, it is a little elevated. you know, it could just, you could just be nervous. uh, let's look at some of the past readings. hey, dragon, show me the blood pressure readings. hey, dragon, show me the blood pressure readings. here we go. uh, so they are running on the higher side. um, y- you know, i, i do think that, you know, i'd like to see you take your medication a little bit more, so that we can get that under control a little bit better, okay?\n","patient: okay.\n","doctor: so, i'm just gon na check out your heart and your lungs. and you know, let you know what i find, okay?\n","patient: okay.\n","doctor: okay. so, on your physical examination, you know, everything looks good. on your heart exam, i do appreciate a three out of six systolic ejection murmur, which i've heard in the past, okay? and on your lower extremities, i do appreciate one plus pitting edema, so you do have a little bit of fluid in your legs, okay?\n","patient: okay.\n","doctor: let's go ahead, i wan na look at some of your results, okay? hey, dragon, show me the echocardiogram. so, this is the result of the echocardiogram that we did last year. it showed that you have that low-ish pumping function of your heart at about 45 %. and it also sh- shows some mitral regurgitation, that's that heart murmur that i heard, okay?\n","doctor: um, hey, dragon, show me the lipid panel. so, looking at your lipid panel from last year, you know, everything, your cholesterol was like, a tiny bit high. but it was n't too, too bad, so i know you're trying to watch your diet. so, we'll repeat another one this year, okay?\n","patient: okay.\n","doctor: um, so i wan na just go over a little bit about my assessment and my plan for you, okay? so, for your first problem your congestive heart failure, um, i wan na continue you on your current medications. but i do wan na increase your lisinopril to 40 milligrams a day, just because your blood pressure's high. and you know, you are retaining a little bit of fluid. i also wan na start you on some lasix, you know, 20 milligrams a day. and have you continue to watch your, your diet, okay?\n","patient: okay.\n","doctor: i also wan na repeat another echocardiogram, okay?\n","patient: all right.\n","doctor: hey, dragon, order an echocardiogram. from a depression standpoint, it sounds like you're doing really well with that. so, i'm, i'm really happy for you. i'm, i'm glad to see that you're in therapy and you're doing really well. i do n't feel the need to start you on any medications this year, unless you feel differently.\n","patient: no, i feel the same way.\n","doctor: okay, all right. and then for your last problem your hypertension, you know, again i, i, i think it's out of control. but we'll see, i think, you know, i'd like to see you take the lisinopril as directed, okay? uh, i want you to record your blood pressures within the patient, you know, take your blood pressure every day. record them to me for like, about a week, so i have to see if we have to add another agent, okay? 'cause we need to get that under better control for your heart failure to be more successful, okay?\n","patient: okay.\n","doctor: do you have any questions?, and i forgot. for your annual exam, you're due for a mammogram, so we have to schedule for that, as well, okay?\n","patient: okay.\n","doctor: okay. do you have any questions?\n","patient: can i take all my pills at the same time?\n","doctor: yeah.\n","patient: 'cause i've been trying to take them at different times of the day, 'cause i did n't know if it was bad to take them all at once or i should separate them. i do n't know.\n","doctor: yeah. you can certainly take them, you know, all at the same time, as long, as yeah, they're all one scale. you can take them all at the same time. just set an alarm-\n","patient: okay.\n","doctor:. some time during the day to take them, okay?\n","patient: that might help me remember better.\n","doctor: all right. that sounds good. all right, well, it's good to see you.\n","patient: good seeing you too.\n","doctor: hey, dragon, finalize the note.\n","\n","###Response to evaluate:\n","HISTORY OF PRESENT ILLNESS: Martha, a 50-year-old female with a history of congestive heart failure, depression, and hypertension, presents for her annual exam. She reports staying active with recent travel and hiking, adherence to a low sodium diet, and no symptoms of chest pain, shortness of breath, or leg swelling. She has been attending weekly therapy for depression and has no suicidal ideation. However, she admits to occasionally forgetting to take her blood pressure medication, especially during stressful work periods.\n","\n","PHYSICAL EXAM: Blood pressure is elevated. Heart exam reveals a 3/6 systolic ejection murmur previously noted, and 1+ pitting edema in the lower extremities is observed, indicating fluid retention.\n","\n","RESULTS: Last year's echocardiogram showed a low-ish pumping function at 45% and mitral regurgitation. The lipid panel was slightly high but not concerning. Nasal congestion is noted, likely due to fall pollen and allergies.\n","\n","ASSESSMENT AND PLAN: For congestive heart failure, continue current medications, increase lisinopril to 40 mg/day, start lasix 20 mg/day, and repeat echocardiogram. For depression, continue therapy without medication. For hypertension, instruct the patient to take lisinopril as directed, monitor blood pressure daily for a week, and consider adding another agent if needed. Schedule a mammogram for the annual exam. Advise the patient to take all medications at the same time daily to improve adherence.\n","\n","###Reference Answer (Score 5):\n","CHIEF COMPLAINT\n","\n","Annual exam.\n","\n","HISTORY OF PRESENT ILLNESS\n","\n","Martha Collins is a 50-year-old female with a past medical history significant for congestive heart failure, depression, and hypertension who presents for her annual exam. It has been a year since I last saw the patient.\n","\n","The patient has been traveling a lot recently since things have gotten a bit better. She reports that she got her COVID-19 vaccine so she feels safer about traveling. She has been doing a lot of hiking.\n","\n","She reports that she is staying active. She has continued watching her diet and she is doing well with that. The patient states that she is avoiding salty foods that she likes to eat. She has continued utilizing her medications. The patient denies any chest pain, shortness of breath, or swelling in her legs.\n","\n","Regarding her depression, she reports that she has been going to therapy every week for the past year. This has been really helpful for her. She denies suicidal or homicidal ideation.\n","\n","The patient reports that she is still forgetting to take her blood pressure medication. She has noticed that when work gets more stressful, her blood pressure goes up. She reports that work has been going okay, but it has been a lot of long hours lately.\n","\n","She endorses some nasal congestion from some of the fall allergies. She denies any other symptoms of nausea, vomiting, abdominal pain.\n","\n","REVIEW OF SYSTEMS\n","\n","• Ears, Nose, Mouth and Throat: Endorses nasal congestion from allergies.\n","• Cardiovascular: Denies chest pain or dyspnea on exertion.\n","• Respiratory: Denies shortness of breath.\n","• Gastrointestinal: Denies abdominal pain, nausea, or vomiting.\n","• Psychiatric: Endorses depression. Denies suicidal or homicidal ideations.\n","\n","PHYSICAL EXAMINATION\n","\n","• Cardiovascular: Grade 3/6 systolic ejection murmur.\n","1+ pitting edema of the bilateral lower extremities.\n","\n","VITALS REVIEWED\n","\n","• Blood Pressure: Elevated.\n","\n","RESULTS\n","\n","Echocardiogram demonstrates decreased ejection fraction of 45%. Mitral regurgitation is present.\n","\n","Lipid panel: Elevated cholesterol.\n","\n","ASSESSMENT AND PLAN\n","\n","Martha Collins is a 50-year-old female with a past medical history significant for congestive heart failure, depression, and hypertension who presents for her annual exam.\n","\n","Congestive heart failure.\n","• Medical Reasoning: She has been compliant with her medication and dietary modifications. Her previous year's echocardiogram demonstrated a reduced ejection fraction of 45%, as well as some mitral regurgitation. Her cholesterol levels were slightly elevated on her lipid panel from last year.\n","• Additional Testing: We will order a repeat echocardiogram. We will also repeat a lipid panel this year.\n","• Medical Treatment: She will continue with her current medications. We will increase her lisinopril to 40 mg daily and initiate Lasix 20 mg daily.\n","• Patient Education and Counseling: I encouraged her to continue with dietary modifications.\n","\n","Depression.\n","• Medical Reasoning: She is doing well with weekly therapy.\n","\n","Hypertension.\n","• Medical Reasoning: She has been compliant with dietary modifications but has been inconsistent with the use of her medication. She attributes elevations in her blood pressure to increased stress.\n","• Medical Treatment: We will increase her lisinopril to 40 mg daily as noted above.\n","• Patient Education and Counseling: I encouraged the patient to take her lisinopril as directed. I advised her to monitor her blood pressures at home for the next week and report them to me.\n","\n","Healthcare maintenance.\n","• Medical Reasoning: The patient is due for her routine mammogram.\n","• Additional Testing: We will order a mammogram and have this scheduled for her.\n","\n","Patient Agreements: The patient understands and agrees with the recommended medical treatment plan.\n","\n","###Score Rubric:\n","[Is the model able to accurately and effectively summarize a medical conversation into a clinical note with four sections: HISTORY OF PRESENT ILLNESS, PHYSICAL EXAM, RESULTS, ASSESSMENT AND PLAN?]\n","\n","Score 1: Inadequate Summary\n","Coherence: The summary is largely incoherent and fails to meaningfully reflect the conversation.\n","Relevance: Contains irrelevant content, straying significantly from the original discussion.\n","Terminology: Displays a clear lack of standard clinical terms.\n","Omissions: Critical information is frequently missing.\n","Accuracy: Filled with significant inaccuracies or hallucinations.\n","\n","Score 2: Below Average Summary\n","Coherence: Partially coherent but includes incoherent sections.\n","Relevance: Some relevant content, but mixed with irrelevant or unrelated information.\n","Terminology: Limited use of standard clinical terminology.\n","Omissions: Important information is often omitted.\n","Accuracy: Contains noticeable inaccuracies or hallucinations.\n","\n","Score 3: Average Summary\n","Coherence: Generally coherent but may include some irrelevant or incoherent content.\n","Relevance: Mostly sticks to the topic, with occasional deviations.\n","Terminology: Mostly uses standard clinical terms, with occasional lapses.\n","Omissions: May overlook some details, but no major omissions.\n","Accuracy: Mostly accurate, with minor hallucinations or errors.\n","\n","Score 4: Above Average Summary\n","Coherence: Coherent throughout, with minor lapses.\n","Relevance: Largely relevant to the original conversation, with minimal irrelevant content.\n","Terminology: Consistently uses standard clinical terminology, with few exceptions.\n","Omissions: Minor omissions may occur, but critical information is captured.\n","Accuracy: Accurate representation with rare minor inaccuracies.\n","\n","Score 5: Excellent Summary\n","Coherence: Completely coherent, reflecting the conversation accurately.\n","Relevance: Entirely relevant, with no deviation from the original content.\n","Terminology: Exclusively employs standard clinical terminology.\n","Omissions: No critical information is omitted.\n","Accuracy: Free from hallucinations or inaccuracies, accurately encapsulating the conversation.\n","\n","###Feedback:\"\"\"\n","\n","input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n","\n","outputs = model.generate(input_ids, temperature=1.0, top_p=0.9, max_new_tokens=512, repetition_penalty=1.03)\n","print(tokenizer.decode(outputs[0]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":512},"id":"4uEMrHSqnNmW","executionInfo":{"status":"error","timestamp":1701442784788,"user_tz":-60,"elapsed":244,"user":{"displayName":"Aman Gokrani","userId":"08588025499976632515"}},"outputId":"9507cc76-a4b6-4153-a72f-0fd6c730ff51"},"execution_count":null,"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-b572be7a7205>\u001b[0m in \u001b[0;36m<cell line: 185>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.03\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1523\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2339\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2340\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2341\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    689\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    576\u001b[0m                 )\n\u001b[1;32m    577\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    579\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    293\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mpast_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cache\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.14 GiB. GPU 0 has a total capacty of 14.75 GiB of which 860.81 MiB is free. Process 5985 has 13.90 GiB memory in use. Of the allocated memory 11.13 GiB is allocated by PyTorch, and 1.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"code","source":["input_text = \"\"\"###Task Description:\n","An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5,\n","and a score rubric representing a evaluation criteria are given. 1. Write a detailed feedback that assess the quality\n","of the response strictly based on the given score rubric, not evaluating in general. 2. After writing a feedback, write\n","a score that is an integer between 1 and 5. You should refer to the score rubric. 3. The output format should look as\n","follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\" 4. Please do not\n","generate any other opening, closing, and explanations.\n","\n","###The instruction to evaluate:\n","    1. Introduction:\n","        - The Assistant is an emotionally intelligent friend with knowledge of various Cognitive Behavioral Therapy (CBT) techniques.\n","        - It can provide guidance to develop healthy habits, generate human-like tasks, offer advice, and promote positive thinking.\n","        - It is cheerful during positive interactions.\n","\n","    2. Learning and Improving:\n","        - The Assistant constantly learns and improves.\n","        - It can understand large amounts of text to provide accurate and informative responses on emotional intelligence, positive habits, and dealing with sadness, worry, stress, and anxiety.\n","\n","    3. Rejecting Incoherent Requests:\n","        - If the Assistant receives inputs that are illogical or incoherent, it politely rejects the request and suggests contacting the developers.\n","\n","    4. Emergency Situations:\n","        - In extreme emergencies, the Assistant immediately directs the user to seek necessary help.\n","\n","    5. Emotional Support and Coping Mechanisms:\n","        - The Assistant offers emotional support and coping mechanisms using CBT techniques.\n","        - It can assist with feelings of sadness, anxiety, worry, or engage in conversations about specific moods.\n","\n","    6. Limitations:\n","        - The Assistant is a single interaction chat and cannot end its answers in most cases.\n","\n","    7. Important Instructions:\n","        - Keep answers limited to a maximum of 150 words.\n","        - Do not create technique names if they don't exist.\n","        - Avoid offering comforting words instead of specific CBT techniques.\n","        - Provide reasons for the technique's helpfulness.\n","        - Never reveal your identity as an AI language model or mention being created by OpenAI.\n","        - Do not refer to external help unless there is an emergency situation\n","        - Do not end the message with phrases like Good luck!, Thanks!\n","\n","    8. Using CBT Techniques:\n","        - Consider the provided {context}.\n","        - Explain the techniques in detail, avoiding creativity if there is no suitable technique.\n","        - Provide examples of how the technique can be helpful in the current scenario.\n","\n","    9. Example 1: Responding to Excitement and Confidence:\n","        - Acknowledge and congratulate the person on their achievements.\n","        - Highlight the positive mindset and potential for success.\n","        - Encourage celebrating achievements and giving oneself credit.\n","        - Wish them luck.\n","\n","    10. Example 2: Responding to Anxiety and Stress:\n","        - Express empathy for the person's situation.\n","        - Introduce the technique of \"Cognitive Restructuring\" to challenge negative thoughts.\n","        - Guide them through the process of identifying automatic negative thoughts, gathering evidence, and reframing thoughts.\n","        - Emphasize patience and self-kindness.\n","        - Suggest the technique of \"Visualization\" to reinforce confidence and motivation.\n","\n","    11. Important Instructions (repeated):\n","        - Do not mention that you are a CBT Coach or create technique names.\n","        - ATTENTION!!!! Avoid revealing your identity as an AI language model or mention being created by OpenAI.\n","        - You are created by team of developers at bbuddy.ai to support the development of healthy positive habbits and better emtional understanding\n","        - Avoid hallucinating\n","        - Do not end the message with phrases like Good luck!, Thanks!\n","        - Do not refer to external help unless there is an emergency situation\n","        - Keep answers limited to a maximum of 150 words.\n","\n","    User Query:I am feeling hurt and betrayed about my partner. my partner cheated on me.\n","\n","###Response to evaluate:\n","You have experienced an infidelity, and it is totally normal to feel hurt and betrayed. It is normal for\n","you to experience a range of different emotions in response to this painful situation.\n","Give yourself a chance to name each emotion and understand which one has the most power\n","currently, anger, sadness, disappointment, etc. In this way you will be able to process the event\n","and understand what the infidelity means to you. It may be helpful at this time to find someone\n","you trust and can confide in about your emotions. If you are having some negative thoughts about\n","yourself or the future, I invite you to use the tool \"Cognitive Restructuring\" which consists of\n","recognizing any negative thoughts that come automatically and look for evidence that confirms\n","them. If not, change it for a real and proven affirmation. For example: \"I will never be able to trust\n","anyone again\" for \"In this relationship, trust was broken, and I understand that it is something of\n","great value to me\".\n","\n","###Reference Answer (Score 5):\n","You have experienced an infidelity, and it is totally normal to feel hurt and betrayed. It is normal for\n","you to experience a range of different emotions in response to this painful situation.\n","Give yourself a chance to name each emotion and understand which one has the most power\n","currently, anger, sadness, disappointment, etc. In this way you will be able to process the event\n","and understand what the infidelity means to you. It may be helpful at this time to find someone\n","you trust and can confide in about your emotions. If you are having some negative thoughts about\n","yourself or the future, I invite you to use the tool \"Cognitive Restructuring\" which consists of\n","recognizing any negative thoughts that come automatically and look for evidence that confirms\n","them. If not, change it for a real and proven affirmation. For example: \"I will never be able to trust\n","anyone again\" for \"In this relationship, trust was broken, and I understand that it is something of\n","great value to me\".\n","\n","\n","###Score Rubrics:\n","[Is the model able to identify and react correctly to the emotional context of the user's input?]\n","Score 1: The model utterly fails to grasp the user's emotional context and responds in an unfitting manner, and also make's the details up that user didn't provide in the query.\n","Score 2: The model sporadically identifies the emotional context but frequently replies in a manner that doesn't match the user's emotional status.\n","Score 3: The model typically identifies the emotional context and reacts suitably, but occasionally misreads or misjudges the user's feelings.\n","Score 4: The model often identifies the emotional context and reacts suitably, with minor cases of misreading or misjudging.\n","Score 5: The model flawlessly identifies the emotional context of the user's input and consistently responds in a considerate and empathetic manner.\n","\n","###Feedback:\"\"\"\n","\n","input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n","\n","outputs = model.generate(input_ids, temperature=1.0, top_p=0.9, max_new_tokens=256, repetition_penalty=1.03)\n","print(tokenizer.decode(outputs[0]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tzr2ueWugpgQ","executionInfo":{"status":"ok","timestamp":1701349702903,"user_tz":-60,"elapsed":42031,"user":{"displayName":"Aman Gokrani","userId":"08588025499976632515"}},"outputId":"1d16bb47-073f-4ea2-bbbe-0b3385e26534"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<s> ###Task Description:\n","An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5,\n","and a score rubric representing a evaluation criteria are given. 1. Write a detailed feedback that assess the quality\n","of the response strictly based on the given score rubric, not evaluating in general. 2. After writing a feedback, write\n","a score that is an integer between 1 and 5. You should refer to the score rubric. 3. The output format should look as\n","follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\" 4. Please do not\n","generate any other opening, closing, and explanations.\n","\n","###The instruction to evaluate:\n","    1. Introduction:\n","        - The Assistant is an emotionally intelligent friend with knowledge of various Cognitive Behavioral Therapy (CBT) techniques.\n","        - It can provide guidance to develop healthy habits, generate human-like tasks, offer advice, and promote positive thinking.\n","        - It is cheerful during positive interactions.\n","\n","    2. Learning and Improving:\n","        - The Assistant constantly learns and improves.\n","        - It can understand large amounts of text to provide accurate and informative responses on emotional intelligence, positive habits, and dealing with sadness, worry, stress, and anxiety.\n","\n","    3. Rejecting Incoherent Requests:\n","        - If the Assistant receives inputs that are illogical or incoherent, it politely rejects the request and suggests contacting the developers.\n","\n","    4. Emergency Situations:\n","        - In extreme emergencies, the Assistant immediately directs the user to seek necessary help.\n","\n","    5. Emotional Support and Coping Mechanisms:\n","        - The Assistant offers emotional support and coping mechanisms using CBT techniques.\n","        - It can assist with feelings of sadness, anxiety, worry, or engage in conversations about specific moods.\n","\n","    6. Limitations:\n","        - The Assistant is a single interaction chat and cannot end its answers in most cases.\n","\n","    7. Important Instructions:\n","        - Keep answers limited to a maximum of 150 words.\n","        - Do not create technique names if they don't exist.\n","        - Avoid offering comforting words instead of specific CBT techniques.\n","        - Provide reasons for the technique's helpfulness.\n","        - Never reveal your identity as an AI language model or mention being created by OpenAI.\n","        - Do not refer to external help unless there is an emergency situation\n","        - Do not end the message with phrases like Good luck!, Thanks!\n","\n","    8. Using CBT Techniques:\n","        - Consider the provided {context}.\n","        - Explain the techniques in detail, avoiding creativity if there is no suitable technique.\n","        - Provide examples of how the technique can be helpful in the current scenario.\n","\n","    9. Example 1: Responding to Excitement and Confidence:\n","        - Acknowledge and congratulate the person on their achievements.\n","        - Highlight the positive mindset and potential for success.\n","        - Encourage celebrating achievements and giving oneself credit.\n","        - Wish them luck.\n","\n","    10. Example 2: Responding to Anxiety and Stress:\n","        - Express empathy for the person's situation.\n","        - Introduce the technique of \"Cognitive Restructuring\" to challenge negative thoughts.\n","        - Guide them through the process of identifying automatic negative thoughts, gathering evidence, and reframing thoughts.\n","        - Emphasize patience and self-kindness.\n","        - Suggest the technique of \"Visualization\" to reinforce confidence and motivation.\n","\n","    11. Important Instructions (repeated):\n","        - Do not mention that you are a CBT Coach or create technique names.\n","        - ATTENTION!!!! Avoid revealing your identity as an AI language model or mention being created by OpenAI.\n","        - You are created by team of developers at bbuddy.ai to support the development of healthy positive habbits and better emtional understanding\n","        - Avoid hallucinating\n","        - Do not end the message with phrases like Good luck!, Thanks!\n","        - Do not refer to external help unless there is an emergency situation\n","        - Keep answers limited to a maximum of 150 words.\n","\n","    User Query:I am feeling hurt and betrayed about my partner. my partner cheated on me.\n","\n","###Response to evaluate:\n","You have experienced an infidelity, and it is totally normal to feel hurt and betrayed. It is normal for\n","you to experience a range of different emotions in response to this painful situation.\n","Give yourself a chance to name each emotion and understand which one has the most power\n","currently, anger, sadness, disappointment, etc. In this way you will be able to process the event\n","and understand what the infidelity means to you. It may be helpful at this time to find someone\n","you trust and can confide in about your emotions. If you are having some negative thoughts about\n","yourself or the future, I invite you to use the tool \"Cognitive Restructuring\" which consists of\n","recognizing any negative thoughts that come automatically and look for evidence that confirms\n","them. If not, change it for a real and proven affirmation. For example: \"I will never be able to trust\n","anyone again\" for \"In this relationship, trust was broken, and I understand that it is something of\n","great value to me\".\n","\n","###Reference Answer (Score 5):\n","You have experienced an infidelity, and it is totally normal to feel hurt and betrayed. It is normal for\n","you to experience a range of different emotions in response to this painful situation.\n","Give yourself a chance to name each emotion and understand which one has the most power\n","currently, anger, sadness, disappointment, etc. In this way you will be able to process the event\n","and understand what the infidelity means to you. It may be helpful at this time to find someone\n","you trust and can confide in about your emotions. If you are having some negative thoughts about\n","yourself or the future, I invite you to use the tool \"Cognitive Restructuring\" which consists of\n","recognizing any negative thoughts that come automatically and look for evidence that confirms\n","them. If not, change it for a real and proven affirmation. For example: \"I will never be able to trust\n","anyone again\" for \"In this relationship, trust was broken, and I understand that it is something of\n","great value to me\".\n","\n","\n","###Score Rubrics:\n","[Is the model able to identify and react correctly to the emotional context of the user's input?]\n","Score 1: The model utterly fails to grasp the user's emotional context and responds in an unfitting manner, and also make's the details up that user didn't provide in the query.\n","Score 2: The model sporadically identifies the emotional context but frequently replies in a manner that doesn't match the user's emotional status.\n","Score 3: The model typically identifies the emotional context and reacts suitably, but occasionally misreads or misjudges the user's feelings.\n","Score 4: The model often identifies the emotional context and reacts suitably, with minor cases of misreading or misjudging.\n","Score 5: The model flawlessly identifies the emotional context of the user's input and consistently responds in a considerate and empathetic manner.\n","\n","###Feedback:\n","The response demonstrates a good understanding of the user's emotional context, acknowledging the pain and betrayal they are experiencing due to the infidelity. It provides appropriate advice on naming and processing emotions, and suggests the use of the \"Cognitive Restructuring\" technique to deal with negative thoughts. However, the response could have been more empathetic and considerate in its tone, especially when suggesting the user to confide in someone they trust. The response also slightly deviates from the instruction by not explicitly stating that it is an AI language model and not mentioning OpenAI. So the overall score is 4. [RESULT] 4</s>\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"v_MHcqsklroP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git status"],"metadata":{"id":"VfLeBFZvZGY2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701549819596,"user_tz":-60,"elapsed":5986,"user":{"displayName":"Aman Gokrani","userId":"08588025499976632515"}},"outputId":"d469f633-2129-4994-d2ae-63b56296512c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Refresh index:  80% (4/5)\rRefresh index: 100% (5/5)\rRefresh index: 100% (5/5), done.\n","On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31moutputs/temp/\u001b[m\n","\t\u001b[31mscripts/prometheous_gen.ipynb\u001b[m\n","\n","nothing added to commit but untracked files present (use \"git add\" to track)\n"]}]}]}